{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Download the database\n",
    "import urllib.request\n",
    "import os \n",
    "import tarfile\n",
    "\n",
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "filepath=\"data/aclImdb_v1.tar.gz\"\n",
    "if not os.path.isfile(filepath):\n",
    "    result = urllib.request.urlretrieve(url,filepath)\n",
    "    print('download:', result)\n",
    "    \n",
    "if not os.path.exists(\"data/aclImb\"):\n",
    "    tfile = tarfile.open(\"data/aclImdb_v1.tar.gz\",'r:gz')\n",
    "    result = tfile.extractall('data/') #解壓縮\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "\n",
    "#remove HTML label\n",
    "def rm_tags(text):\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('',text)\n",
    "\n",
    "\n",
    "#loading IMDb files\n",
    "import os\n",
    "def read_files(filetype):\n",
    "    path = \"data/aclImdb/\"\n",
    "    file_list=[]\n",
    "    positive_path = path+filetype+\"/pos/\"\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list+=[positive_path+f]\n",
    "        \n",
    "    negative_path = path+filetype+\"/neg/\"\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list+=[negative_path+f]   \n",
    "    \n",
    "    print('read',filetype, 'files', len(file_list))\n",
    "    all_labels = ([1]*12500+[0]*12500)\n",
    "    all_texts  = []\n",
    "    for fi in file_list:\n",
    "        with open(fi, encoding='utf8') as file_input:\n",
    "            all_texts += [rm_tags(\" \".join(file_input.readlines()))] #conect file after remove HTNL \n",
    "            \n",
    "    return all_labels,all_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train files 25000\n",
      "read test files 25000\n"
     ]
    }
   ],
   "source": [
    "y_train, train_text = read_files(\"train\")\n",
    "y_test , test_text  = read_files(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[0] is pos or neg =  1\n",
      "train_text[0]:\n",
      "y_train[12501] is pos or neg =  0\n",
      "train_text[12501]:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "print('y_train[0] is pos or neg = ',y_train[0])\n",
    "print('train_text[0]:')\n",
    "train_text[0]\n",
    "\n",
    "print('y_train[12501] is pos or neg = ',y_train[12501])\n",
    "print('train_text[12501]:')\n",
    "train_text[12501]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pad_sequences length =  106\n",
      "[308, 6, 3, 1068, 208, 8, 29, 1, 168, 54, 13, 45, 81, 40, 391, 109, 137, 13, 57, 149, 7, 1, 481, 68, 5, 260, 11, 6, 72, 5, 631, 70, 6, 1, 5, 1, 1530, 33, 66, 63, 204, 139, 64, 1229, 1, 4, 1, 222, 899, 28, 68, 4, 1, 9, 693, 2, 64, 1530, 50, 9, 215, 1, 386, 7, 59, 3, 1470, 798, 5, 176, 1, 391, 9, 1235, 29, 308, 3, 352, 343, 142, 129, 5, 27, 4, 125, 1470, 5, 308, 9, 532, 11, 107, 1466, 4, 57, 554, 100, 11, 308, 6, 226, 47, 3, 11, 8, 214]\n"
     ]
    }
   ],
   "source": [
    "#build token\n",
    "token = Tokenizer(num_words = 2000)\n",
    "token.fit_on_texts(train_text)\n",
    "\n",
    "#print(token.document_count)\n",
    "#print(token.word_index)\n",
    "\n",
    "#token\n",
    "x_train_seq = token.texts_to_sequences(train_text)\n",
    "x_test_seq  = token.texts_to_sequences(test_text)\n",
    "\n",
    "#print(train_text[0])\n",
    "#print(x_train_seq[0])\n",
    "\n",
    "#same length\n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=100)\n",
    "x_test  = sequence.pad_sequences(x_test_seq , maxlen=100)\n",
    "\n",
    "print('before pad_sequences length = ', len(x_train_seq[0]))\n",
    "print(x_train_seq[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "4s - loss: 0.4773 - acc: 0.7558 - val_loss: 0.4363 - val_acc: 0.8006\n",
      "Epoch 2/10\n",
      "4s - loss: 0.2676 - acc: 0.8913 - val_loss: 0.4232 - val_acc: 0.8148\n",
      "Epoch 3/10\n",
      "4s - loss: 0.1662 - acc: 0.9379 - val_loss: 0.7306 - val_acc: 0.7294\n",
      "Epoch 4/10\n",
      "4s - loss: 0.0848 - acc: 0.9701 - val_loss: 0.7728 - val_acc: 0.7614\n",
      "Epoch 5/10\n",
      "4s - loss: 0.0532 - acc: 0.9813 - val_loss: 0.9236 - val_acc: 0.7596\n",
      "Epoch 6/10\n",
      "4s - loss: 0.0380 - acc: 0.9857 - val_loss: 1.0134 - val_acc: 0.7662\n",
      "Epoch 7/10\n",
      "4s - loss: 0.0301 - acc: 0.9890 - val_loss: 1.1382 - val_acc: 0.7700\n",
      "Epoch 8/10\n",
      "4s - loss: 0.0310 - acc: 0.9882 - val_loss: 1.1488 - val_acc: 0.7700\n",
      "Epoch 9/10\n",
      "4s - loss: 0.0302 - acc: 0.9893 - val_loss: 1.2833 - val_acc: 0.7440\n",
      "Epoch 10/10\n",
      "4s - loss: 0.0236 - acc: 0.9923 - val_loss: 1.2223 - val_acc: 0.7630\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=32,input_dim=2000,input_length=100))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=256,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "#model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "train_history = model.fit(x_train,y_train,batch_size=100,epochs=10,verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24128/25000 [===========================>..] - ETA: 0sthe scores is :  0.81364\n",
      "24640/25000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "#scores & predict\n",
    "scores = model.evaluate(x_test,y_test,verbose=1)\n",
    "print('the scores is : ',scores[1])\n",
    "\n",
    "predict = model.predict_classes(x_test)\n",
    "#predict[:10]\n",
    "\n",
    "predict_classes = predict.reshape(-1)\n",
    "#predict_classes[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#數值轉換文字- function\n",
    "SentimentDict={1:'正面的',0:'負面的'}\n",
    "def display_test_sentiment(i):\n",
    "    print(test_text[i])\n",
    "    print('label真實值:', SentimentDict[y_test[i]],'預測結果:',SentimentDict[predict_classes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a recreational golfer with some knowledge of the sport's history, I was pleased with Disney's sensitivity to the issues of class in golf in the early twentieth century. The movie depicted well the psychological battles that Harry Vardon fought within himself, from his childhood trauma of being evicted to his own inability to break that glass ceiling that prevents him from being accepted as an equal in English golf society. Likewise, the young Ouimet goes through his own class struggles, being a mere caddie in the eyes of the upper crust Americans who scoff at his attempts to rise above his standing. What I loved best, however, is how this theme of class is manifested in the characters of Ouimet's parents. His father is a working-class drone who sees the value of hard work but is intimidated by the upper class; his mother, however, recognizes her son's talent and desire and encourages him to pursue his dream of competing against those who think he is inferior.Finally, the golf scenes are well photographed. Although the course used in the movie was not the actual site of the historical tournament, the little liberties taken by Disney do not detract from the beauty of the film. There's one little Disney moment at the pool table; otherwise, the viewer does not really think Disney. The ending, as in \"Miracle,\" is not some Disney creation, but one that only human history could have written.\n",
      "label真實值: 正面的 預測結果: 正面的\n"
     ]
    }
   ],
   "source": [
    "#顯示測試的評論\n",
    "display_test_sentiment(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      " 此影評為 正面的\n"
     ]
    }
   ],
   "source": [
    "#預測影評\n",
    "\n",
    "#build function\n",
    "def predict_review(input_text):\n",
    "    input_seq= token.texts_to_sequences([input_text]) #text to list\n",
    "    pad_input_seq = sequence.pad_sequences(input_seq, maxlen=100) #limit text number\n",
    "    predict_result = model.predict_classes([pad_input_seq])#strat predict\n",
    "    print(' 此影評為',SentimentDict[predict_result[0][0]])#\n",
    "    \n",
    "    #print('input_text')\n",
    "    #print(input_seq[0])\n",
    "    #print('input_text length = ', len(input_text ),'\\n' )\n",
    "    \n",
    "#predict new text from web\n",
    "input_text = 'I loved this Movie I love The Cast I love Emma Watson I Love The Movie A Lot Beauty And Beast Movie Will Be In My Favorite List Always I Love the Movie Very Much In The Movie Emma Watson Was Looking Very beautiful Her character was So good In the film i am always a greatest fan of Emma Watson I love All the movie in which she act She always acting very beautifully.'\n",
    "predict_review(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 100, 32)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_7 (SimpleRNN)     (None, 16)                784       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               4352      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 69,393\n",
      "Trainable params: 69,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "4s - loss: 0.5435 - acc: 0.7211 - val_loss: 0.4421 - val_acc: 0.8126\n",
      "Epoch 2/10\n",
      "3s - loss: 0.3714 - acc: 0.8423 - val_loss: 0.5619 - val_acc: 0.7596\n",
      "Epoch 3/10\n",
      "3s - loss: 0.3377 - acc: 0.8593 - val_loss: 0.6135 - val_acc: 0.7460\n",
      "Epoch 4/10\n",
      "3s - loss: 0.3142 - acc: 0.8692 - val_loss: 0.5572 - val_acc: 0.7660\n",
      "Epoch 5/10\n",
      "3s - loss: 0.2829 - acc: 0.8882 - val_loss: 0.6241 - val_acc: 0.7394\n",
      "Epoch 6/10\n",
      "3s - loss: 0.2463 - acc: 0.9038 - val_loss: 0.5633 - val_acc: 0.7868\n",
      "Epoch 7/10\n",
      "3s - loss: 0.2268 - acc: 0.9091 - val_loss: 0.6916 - val_acc: 0.7264\n",
      "Epoch 8/10\n",
      "3s - loss: 0.2027 - acc: 0.9206 - val_loss: 0.7290 - val_acc: 0.7326\n",
      "Epoch 9/10\n",
      "3s - loss: 0.1814 - acc: 0.9276 - val_loss: 0.7386 - val_acc: 0.7434\n",
      "Epoch 10/10\n",
      "3s - loss: 0.1654 - acc: 0.9363 - val_loss: 0.6044 - val_acc: 0.8114\n",
      "24992/25000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81976000000000004"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#special RNN MODEL for times\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=32,input_dim=2000,input_length=100))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(SimpleRNN(units=16))\n",
    "model.add(Dense(units=256,activation='relu'))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "train_history = model.fit(x_train,y_train,batch_size=100,epochs=10,verbose=2,validation_split=0.2)\n",
    "scores = model.evaluate(x_test,y_test,verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 100, 32)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 81,025\n",
      "Trainable params: 81,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "19s - loss: 0.4824 - acc: 0.7593 - val_loss: 0.4760 - val_acc: 0.7660\n",
      "Epoch 2/10\n",
      "18s - loss: 0.3235 - acc: 0.8643 - val_loss: 0.5572 - val_acc: 0.7294\n",
      "Epoch 3/10\n",
      "19s - loss: 0.3006 - acc: 0.8749 - val_loss: 0.5324 - val_acc: 0.7482\n",
      "Epoch 4/10\n",
      "19s - loss: 0.2877 - acc: 0.8805 - val_loss: 0.4043 - val_acc: 0.8318\n",
      "Epoch 5/10\n",
      "20s - loss: 0.2722 - acc: 0.8872 - val_loss: 0.3627 - val_acc: 0.8360\n",
      "Epoch 6/10\n",
      "20s - loss: 0.2526 - acc: 0.8965 - val_loss: 0.4229 - val_acc: 0.8014\n",
      "Epoch 7/10\n",
      "20s - loss: 0.2436 - acc: 0.8994 - val_loss: 0.4346 - val_acc: 0.8248\n",
      "Epoch 8/10\n",
      "22s - loss: 0.2315 - acc: 0.9064 - val_loss: 0.4470 - val_acc: 0.8284\n",
      "Epoch 9/10\n",
      "24s - loss: 0.2206 - acc: 0.9125 - val_loss: 0.4250 - val_acc: 0.8098\n",
      "Epoch 10/10\n",
      "23s - loss: 0.2132 - acc: 0.9143 - val_loss: 0.6817 - val_acc: 0.7368\n",
      "25000/25000 [==============================] - 12s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82264000000000004"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#special LSTM MODEL for times\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=32,input_dim=2000,input_length=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(units=256,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "train_history = model.fit(x_train,y_train,batch_size=100,epochs=10,verbose=2,validation_split=0.2)\n",
    "scores = model.evaluate(x_test,y_test,verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
