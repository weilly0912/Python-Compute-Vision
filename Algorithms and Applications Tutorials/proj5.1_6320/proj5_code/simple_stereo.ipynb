{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Depth Estimation using Stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple stereo by matching patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that there is some encoding of depth when the images are captured using a stereo rig, much like human eyes.\n",
    "\n",
    "\n",
    "You can try a simple experiment to see the stereo effect in action. Try seeing a scene with only your left eye. Then close your left eye and see using your right eye. Make the transition quickly. You should notice a horizontal shift in the image perceived. Can you comment on the difference in shift for different objects when you do this experiment? Is it related to the depth of the objects in some way?\n",
    "\n",
    "\n",
    "In this notebook, we will generate disparity maps, which is the map of horizontal shifts estimated at each pixel. We will start working on a simple algorithm which will then be evolved to give better disparity maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install Miniconda. It doesn’t matter whether you use Python 2 or 3 because we will create our own environment that uses 3 anyways.\n",
    "2. Create a conda environment using the appropriate command. On Windows, open the installed “Conda prompt” to run the command. On MacOS and Linux, you can just use a terminal window to run the command, Modify the command based on your OS (linux, mac, or win): `conda env create -f proj5_env_<OS>.yml`\n",
    "3. This should create an environment named ‘proj5’. Activate it using the Windows command, activate proj5 or the MacOS / Linux command, source activate proj5\n",
    "4. Install the project package, by running `pip install -e .` inside the repo folder.\n",
    "5. Run the notebook using `jupyter notebook ./proj5_code/simple_stereo.ipynb`\n",
    "6. Ensure that all sanity checks are passing by running pytest inside the “unit_tests/” folder.\n",
    "7. Generate the zip folder for the code portion of your submission once you’ve finished the project using `python zip_submission.py --username <your_uid>` and submit to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup\n",
    "\n",
    "For this project, you must do a project report using the template slides provided to you. Do not change the order of the slides or remove any slides, as this will affect the grading process on Gradescope and you will be deducted points. In the report you will describe your algorithm and any decisions you made to write your algorithm a particular way. Then you will show and discuss the results of your algorithm. The template slides provide guidance for what you should include in your report. A good writeup doesn’t just show results–it tries to draw some conclusions from the experiments. You must convert the slide deck into a PDF for your submission.\n",
    "\n",
    "If you choose to do anything extra, add slides after the slides given in the template deck to describe your implementation, results, and analysis. Adding slides in between the report template will cause issues with Gradescope, and you will be deducted points. You will not receive full credit for your extra credit implementations if they are not described adequately in your writeup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric (Total : 100 pts)\n",
    "* 60 pts: Code\n",
    "    * 15 pts: generate_random_stereogram in utils.py\n",
    "    * 15 pts: similarity_measures.py\n",
    "    * 30 pts: disparity_map.py\n",
    "* 40 pts: Report\n",
    "* -5*n pts: Lose 5 points for every time you do not follow the instructions for the hand-in format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from proj5_code.utils import load_image, PIL_resize, generate_random_stereogram, stereo_helper_fn\n",
    "from proj5_code.disparity_map import calculate_disparity_map\n",
    "from proj5_code.similarity_measures import ssd_similarity_measure, sad_similarity_measure\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unit_tests.test_base import verify\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a helper function called stereo_helper_fn for utils.py for calculating and plotting the disparity maps using the functions defined by you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random dot stereogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was once believed that in order to perceive depth, one must either match feature points (like SIFT) between left and right images, or rely upon cues such as shadows.\n",
    "\n",
    "A random dot stereogram eliminates all other depth cues and hence it proves that a stereo setup is sufficient to get an idea of the depth of the scene.\n",
    "\n",
    "A random dot stereogram is generated by the follow steps:\n",
    "1. Create the left image with random dots at each pixel (0/1 values).\n",
    "2. Create the right image as the copy of left image.\n",
    "3. Select a region in the right image and shift it horizontally.\n",
    "4. Add a random pattern in the right image in the empty region created after the shift.\n",
    "\n",
    "You will implement these steps in the function `generate_random_stereogram()` in `utils.py`. A corresponding unit test is defined in test_utils.py.\n",
    "\n",
    "Please read the documentation carefully.\n",
    "\n",
    "#### Reflection Question:\n",
    "1. What do you think of the random dot stereogram? Can you judge the depth by looking at the images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate left and right images\n",
    "im_left, im_right = generate_random_stereogram(im_size=(51, 51, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unit_tests.test_utils import test_generate_random_stereogram\n",
    "\n",
    "print('Test for random dot stereogram', verify(test_generate_random_stereogram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity measure\n",
    "\n",
    "\n",
    "We will use a similarity function to compare patches between left and right images. We will implement two similarity functions:\n",
    "\n",
    "1. Sum of squared differences (SSD): $SSD\\left(A,B\\right) = \\sum_{i,j}\\left(A_{ij} - B_{ij}\\right)^2$\n",
    "2. Sum of absolute differences (SAD): $SAD\\left(A,B\\right) = \\sum_{i,j}|A_{ij} - B_{ij}|$\n",
    "\n",
    "You will implement these functions in `similarity_measures.py`. The corresponding unit tests are defined in `test_similarity_measures.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the similarity function and disparity map calculation. You will need it in the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unit_tests.test_similarity_measures import (\n",
    "  test_ssd_similarity_measure_values, \n",
    "  test_sad_similarity_measure_values, \n",
    "  test_similarity_measure_size_compatibility\n",
    ")\n",
    "\n",
    "print('Testing value for SAD measure', verify(test_sad_similarity_measure_values))\n",
    "print('Testing value for SSD measure', verify(test_ssd_similarity_measure_values))\n",
    "print('Testing input size compatibility for measures', verify(test_similarity_measure_size_compatibility))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disparity Map\n",
    "\n",
    "We are now ready to write the code for a simple algorithm for stereo matching.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figures/disparity_calculation_example.jpg\" width=\"600\" title=\"Example of a stereo algorithm\" />\n",
    "    <center><figcaption align=\"center\">Example of a stereo algorithm</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "These are the steps taken in this image (and will be implemented by you):\n",
    "\n",
    "1. Pick a patch in the left image (red block), P1.\n",
    "2. Place the patch in the same (x,y) coordinates in the right image (red block). As this is binocular stereo, we will need to search for P1 on the left side starting from this position. Make sure you understand this point well before proceeding further.\n",
    "4. Slide the block of candidates to the left (indicated by the different pink blocks). The search area is restricted by the parameter max_search_bound in the code. The candidates will overlap.\n",
    "5. We will pick the candidate patch with the minimum similarity error (green block). The horizontal shift from the red block to the green block in this image is the disparity value for the centre of P1 in the left image.\n",
    "\n",
    "Note: the images have already been rectified and hence we can search on just a horizontal scan line.\n",
    "\n",
    "The function works as follows:\n",
    "\n",
    "* Input\n",
    "    1. Left image\n",
    "    2. Right image\n",
    "    3. Similarity function\n",
    "    4. Patch size\n",
    "    5. Max search value\n",
    "* Output\n",
    "    1. Disparity map\n",
    "\n",
    "Implement this in `disparity_map.py` (please read the documentation carefully!). The corresponding unit tests are defined in `test_disparity_map.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverables**:\n",
    "All the disparity maps\n",
    "+\n",
    "Observations and Analysis:\n",
    "\n",
    "#### Reflection Question\n",
    "1. What is the effect of increasing the block size? \n",
    "2. Why is the result poor on the left edge and not on the other edges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unit_tests.test_disparity_map import (\n",
    "  test_disparity_deltafn_failure,\n",
    "  test_disparity_deltafn_success,\n",
    "  test_disparity_map_size,\n",
    "  test_disparity_random_stereogram,\n",
    "  test_disparity_translation_shift\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing for disparity map on a delta function', verify(test_disparity_deltafn_failure))\n",
    "print('Testing for disparity map on a delta function', verify(test_disparity_deltafn_success))\n",
    "print('Testing disparity map size', verify(test_disparity_map_size))\n",
    "print('Testing random stereogram ouptut', verify(test_disparity_random_stereogram))\n",
    "print('Testing disparity on translation shift', verify(test_disparity_translation_shift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_helper_fn(im_left, im_right, block_size = [3,5,9,13], max_search_bound=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error profile analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the error profile analysis, you have to find two examples which display convex and non-convex error profile respectively. For reference, these are the plots we obtained:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figures/convex_sample.png\" title=\"Convex Sample\" />\n",
    "    <center><figcaption align = \"center\">Convex Profile</figcaption></center>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"figures/non_convex_sample.png\" title=\"Non Convex Sample\" />\n",
    "    <center><figcaption align = \"center\">Non-Convex Profile</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Before computing the full disparity map, we will analyse the similarity error between patches. You will have to find out different patches in the image which exhibit a close-to-convex error profile, and a highly non-convex profile.\n",
    "\n",
    "**Deliverable**:\n",
    "Find the patch in the left image and search space in the right image, and the similarity error plot for the two cases, and copy it to the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "base_path = '../data/adirondack/'\n",
    "im_left = PIL_resize(load_image(base_path + 'im_left.png'), (0.1, 0.1))\n",
    "im_right = PIL_resize(load_image(base_path + 'im_right.png'), (0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,10))\n",
    "\n",
    "ax1.imshow(im_left, interpolation=None)\n",
    "ax1.title.set_text('Left image')\n",
    "ax1.autoscale(False)\n",
    "ax1.set_axis_on()\n",
    "\n",
    "ax2.imshow(im_right, interpolation=None)\n",
    "ax2.title.set_text('Right image')\n",
    "ax2.autoscale(False)\n",
    "ax2.set_axis_on()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex error profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract a patch of interest from the left image\n",
    "patch_size=15\n",
    "x_idx, y_idx = (None, None) # TODO: replace with integers\n",
    "patch_left_img = torch.tensor(im_left[x_idx:x_idx+patch_size+1, y_idx:y_idx+patch_size+1,:])\n",
    "plt.imshow(patch_left_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the search area in the right image\n",
    "max_search_bound = 0 # might need adjustment based on your (x_idx, y_idx)\n",
    "search_area_right_img = torch.tensor(\n",
    "  im_right[x_idx:x_idx+patch_size+1, y_idx-max_search_bound:y_idx+patch_size+1,:]\n",
    ")\n",
    "plt.imshow(search_area_right_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_vals = np.array(\n",
    "  [sad_similarity_measure(patch_left_img, search_area_right_img[:,h_idx:h_idx+patch_size+1,:]) \n",
    "   for h_idx in range(search_area_right_img.shape[1]-patch_size-1)\n",
    "  ])\n",
    "plt.plot(similarity_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Convex error profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract a patch of interest from the left image\n",
    "patch_size=15\n",
    "x_idx, y_idx = (None, None) # TODO: replace with integers\n",
    "patch_left_img = torch.tensor(im_left[x_idx:x_idx+patch_size+1, y_idx:y_idx+patch_size+1,:])\n",
    "plt.imshow(patch_left_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the search area in the right image\n",
    "max_search_bound = 0 # might need adjustment based on your (x_idx, y_idx)\n",
    "search_area_right_img = torch.tensor(\n",
    "  im_right[x_idx:x_idx+patch_size+1, y_idx-max_search_bound:y_idx+patch_size+1,:]\n",
    ")\n",
    "plt.imshow(search_area_right_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_vals = np.array(\n",
    "  [sad_similarity_measure(patch_left_img, search_area_right_img[:,h_idx:h_idx+patch_size+1,:]) \n",
    "   for h_idx in range(search_area_right_img.shape[1]-patch_size-1)\n",
    "  ])\n",
    "plt.plot(similarity_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real life stereo images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stereo_helper_fn(torch.tensor(im_left), torch.tensor(im_right), max_search_bound=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverables**\n",
    "1. Copy the disparity map for the patch size you feel works the best\n",
    "2. Can you think of an explanation as to why the back rest of the chair appears *blocky*?\n",
    "\n",
    "Tip: you can see all the examples and deliverables before answering. This will help you understand the core ideas being asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data/bicycle/'\n",
    "im_left = PIL_resize(load_image(base_path + 'im_left.png'), (0.1, 0.1))\n",
    "im_right = PIL_resize(load_image(base_path + 'im_right.png'), (0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_helper_fn(torch.tensor(im_left), torch.tensor(im_right), block_size=[11], max_search_bound=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data/bowling/'\n",
    "im_left = PIL_resize(load_image(base_path + 'im_left.png'), (0.2, 0.2))\n",
    "im_right = PIL_resize(load_image(base_path + 'im_right.png'), (0.2, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stereo_helper_fn(torch.tensor(im_left), torch.tensor(im_right), block_size=[9], max_search_bound=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data/bowling2/'\n",
    "im_left = PIL_resize(load_image(base_path + 'im_left.png'), (0.20, 0.20))\n",
    "im_right = PIL_resize(load_image(base_path + 'im_right.png'), (0.20, 0.20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_helper_fn(torch.tensor(im_left), torch.tensor(im_right), block_size=[9], max_search_bound=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverables** of set 3 and set 4 combined:\n",
    "\n",
    "1. Copy the disparity maps from set 4 in the report\n",
    "2. Can you spot a peculiar behaviour of the diaprity maps near the head of bowling pin on the right? What do you see in the input images in that area? Can you figure out the reason behind this behaviour?\n",
    "3. Notice that we have manipulated the images in set 4 to generate set 4 images. This leads to a difference in disparity map results in the bottom of the image and on the green bowling ball. Can you explain why the disparity drops to zero in both these regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data/flowers/'\n",
    "im_left = PIL_resize(load_image(base_path + 'im_left.png'), (0.10, 0.10))\n",
    "im_right = PIL_resize(load_image(base_path + 'im_right.png'), (0.10, 0.10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_helper_fn(torch.tensor(im_left), torch.tensor(im_right), block_size=[9], max_search_bound=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: (these observations do not go in the report. These are for your understanding).\n",
    "\n",
    "1. Notice the different disparity of the flower at the back and its shadow\n",
    "2. Spot the zero-disparity region in the center of the house\n",
    "3. See how smooth the disparity values are on the couch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data/stairs/'\n",
    "im_left = PIL_resize(load_image(base_path + 'im_left.jpg'), (1, 1))\n",
    "im_right = PIL_resize(load_image(base_path + 'im_right.jpg'), (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_helper_fn(torch.tensor(im_left), torch.tensor(im_right), block_size = [3, 5, 7], max_search_bound=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverables**\n",
    "\n",
    "1. Why are we able to see the shift in disparity values on the wall?\n",
    "2. What is the effect of block_size and the ability to see stairs-like structure in the disparity map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the results from is that they aren't very smooth. Pixels next to each other on the same surface can have vastly different disparities, making the results look very noisy and patchy in some areas. Intuitively, pixels next to each other should have a smooth transition in disparity(unless at an object boundary or occlusion).\n",
    "In this section, we try to improve our results. One way of doing this is through the use of a smoothing constraint. The smoothing method we use is called Semi-Global Matching(SGM) or Semi-Global Block Matching. Before, we picked the disparity for a pixel based on the minimum matching cost of the block using some metric(SSD or SAD). The basic idea of SGM is to penalize pixels with a disparity that's very different than their neighbors by adding a penalty term on top of the matching cost term. SGM tries to minimize the global(over the entire image) energy function\n",
    "\\begin{equation*}\n",
    "E(D) \\leq \\sum_{p} (C(p, D_p) + \\sum_{q} PT(|D_p - D_q|))\n",
    "\\end{equation*}\n",
    "C(p,D_p) is the matching cost for a pixel with disparity D_p, q is a neighboring pixel, and PT is some penalty function penalizing the difference in disparities.\n",
    "You can read more about how this method works and is optimized here:\n",
    "https://elib.dlr.de/73119/1/180Hirschmueller.pdf\n",
    "and\n",
    "https://pdfs.semanticscholar.org/bcd8/4d8bd864ff903e3fe5b91bed3f2eedacc324.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the smoothing algorithm, we need to implement a function which computes the **cost volume**. We have already written code to compute disparity map. We will extend that code to compute the cost volume. Instead of taking the argmin of the similarity error profile, we will store the tensor of error profile at each pixel location along the third dimension.\n",
    "\n",
    "If we have an input image of dimension (H,W,C) and max search bound of D, the cost_volume will be a tensor of dimension (H,W,D). The cost volumne at (i,j) pixel is the error profile obtained for the patch in the left image centered at (i,j).\n",
    "\n",
    "Implement this part as function ```calculate_cost_volume``` in ```disparity_map.py```. Feel free to reuse any code you have written till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unit_tests.test_disparity_map import (\n",
    "  test_calculate_cost_volume\n",
    ")\n",
    "\n",
    "print('Testing for calculate_cost_volume', verify(test_calculate_cost_volume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "from semiglobalmatching.sgm import sgm\n",
    "from scipy import ndimage\n",
    "from proj5_code.similarity_measures import sad_similarity_measure, ssd_similarity_measure\n",
    "\n",
    "#you can change the path to try other pairs, but you may need to fix the scaling per pair\n",
    "base_path = '../data/adirondack/'\n",
    "im_left = PIL_resize(load_image(base_path + 'im_left.png'), (0.10, 0.10))\n",
    "im_right = PIL_resize(load_image(base_path + 'im_right.png'), (0.10, 0.10))\n",
    "\n",
    "#calculates the disparity map with SGM, the last argument is max disparity to consider\n",
    "disparity_map = sgm(im_left,im_right, \"result\", 30, sad_similarity_measure, 9)\n",
    "result = ndimage.median_filter(disparity_map, size=5)\n",
    "plt.figure()\n",
    "plt.imshow(result, cmap='jet', interpolation='nearest')\n",
    "plt.title('Disparity map')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverables**\n",
    "\n",
    "1. Compare these results qualitatively to the output of the chair image without smoothing.\n",
    "2. What regions of the image does smoothing seem to perform better on and why do you think that is?\n",
    "3. What regions of the image does smoothing seem to perform worse on and why do you think that is?\n",
    "4. Would smoothing still work for images with both a horizontal and vertical shift?\n",
    "\n",
    "\n",
    "(Extra Credit: 5 pts)Try the above smoothing with your own image pair! Take 2 images with only(or mostly) a horizontal shift, and see the result by editing the image paths and running the code. If you get good results, explain why your image pair is \"easy\". If you get bad results, explain why your pair is \"hard\". These results go in the extra credit slides. "
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
